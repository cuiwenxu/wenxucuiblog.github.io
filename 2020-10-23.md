A client that consumes records from a Kafka cluster.
  <p>
  This client transparently handles the failure of Kafka brokers, and transparently adapts as topic partitions
  it fetches migrate within the cluster. This client also interacts with the broker to allow groups of
  consumers to load balance consumption using <a href="#consumergroups">consumer groups</a>.
  <p>
  The consumer maintains TCP connections to the necessary brokers to fetch data.
  Failure to close the consumer after use will leak these connections.
  The consumer is not thread-safe. See <a href="#multithreaded">Multi-threaded Processing</a> for more details.
 
  <h3>Cross-Version Compatibility</h3>
  This client can communicate with brokers that are version 0.10.0 or newer. Older or newer brokers may not support
  certain features. For example, 0.10.0 brokers do not support offsetsForTimes, because this feature was added
  in version 0.10.1. You will receive an {@link org.apache.kafka.common.errors.UnsupportedVersionException}
  when invoking an API that is not available on the running broker version.
  <p>
 
  <h3>Offsets and Consumer Position</h3>
  Kafka maintains a numerical offset for each record in a partition. This offset acts as a unique identifier of
  a record within that partition, and also denotes the position of the consumer in the partition. For example, a consumer
  which is at position 5 has consumed records with offsets 0 through 4 and will next receive the record with offset 5. There
  are actually two notions of position relevant to the user of the consumer:
  <p>
  The {@link #position(TopicPartition) position} of the consumer gives the offset of the next record that will be given
  out. It will be one larger than the highest offset the consumer has seen in that partition. It automatically advances
  every time the consumer receives messages in a call to {@link #poll(Duration)}.
  <p>
  The {@link #commitSync() committed position} is the last offset that has been stored securely. Should the
  process fail and restart, this is the offset that the consumer will recover to. The consumer can either automatically commit
  offsets periodically; or it can choose to control this committed position manually by calling one of the commit APIs
  (e.g. {@link #commitSync() commitSync} and {@link #commitAsync(OffsetCommitCallback) commitAsync}).
  <p>
  This distinction gives the consumer control over when a record is considered consumed. It is discussed in further
  detail below.
 
  <h3><a name="consumergroups">Consumer Groups and Topic Subscriptions</a></h3>
 
  Kafka uses the concept of <i>consumer groups</i> to allow a pool of processes to divide the work of consuming and
  processing records. These processes can either be running on the same machine or they can be
  distributed over many machines to provide scalability and fault tolerance for processing. All consumer instances
  sharing the same {@code group.id} will be part of the same consumer group.
  <p>
  Each consumer in a group can dynamically set the list of topics it wants to subscribe to through one of the
  {@link #subscribe(Collection, ConsumerRebalanceListener) subscribe} APIs. Kafka will deliver each message in the
  subscribed topics to one process in each consumer group. This is achieved by balancing the partitions between all
  members in the consumer group so that each partition is assigned to exactly one consumer in the group. So if there
  is a topic with four partitions, and a consumer group with two processes, each process would consume from two partitions.
  <p>
  Membership in a consumer group is maintained dynamically: if a process fails, the partitions assigned to it will
  be reassigned to other consumers in the same group. Similarly, if a new consumer joins the group, partitions will be moved
  from existing consumers to the new one. This is known as <i>rebalancing</i> the group and is discussed in more
  detail <a href="#failuredetection">below</a>. Group rebalancing is also used when new partitions are added
  to one of the subscribed topics or when a new topic matching a {@link #subscribe(Pattern, ConsumerRebalanceListener) subscribed regex}
  is created. The group will automatically detect the new partitions through periodic metadata refreshes and
  assign them to members of the group.
  <p>
  Conceptually you can think of a consumer group as being a single logical subscriber that happens to be made up of
  multiple processes. As a multi-subscriber system, Kafka naturally supports having any number of consumer groups for a
  given topic without duplicating data (additional consumers are actually quite cheap).
  <p>
