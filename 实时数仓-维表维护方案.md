@[TOC](实时数仓-维表维护方案)
# 维表维护方案
维度数据维护是实时数仓搭建的重要一环，接下来说公司维度处理上的方案演化。
## 阶段一
维度数据在mysql维护，flink job使用datastream api开发。实现RichSourceFunction，在invoke方法中执行查询逻辑。这个方案弊端有两点：
- 问题1
公司内Flink job大概有200个，每个job需要关联3到5张维度表。现有的维度表关联是用mysql source，每个维表需要维护一个链接，非常占用mysql的链接资源
- 问题2
基于mysql source的查询是周期性进行的，实际生产中是1h查询一次，此间维度表变化无法及时追踪

同时，flink sql已经迭代了几个大版本，公司也在尝试使用flink sql来完成指标计算。于是演化到阶段二
## 阶段二
一个库下所有维表通过cdc job导入kafka(upsert的方式)，然后使用flink sql解析kafka的数据，将维度表导入hbase，最后在计算时事实表和hbase维度表使用动态表join的方式。架构图如下所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210302150644480.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE2MjQxNTc=,size_16,color_FFFFFF,t_70)

该技术方案主要包含以下几个技术点：
- cdc数据upsert到kafka
- kafka数据sink hbase
- 事件表和hbase维度表关联

# 技术细节代码解析
## cdc数据upsert到kafka
cdc数据通过api方式获取，网上好多代码，此处不赘述，着重看下upsert kafka的实现
```java
package com.xxx;

import com.alibaba.fastjson.JSON;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.runtime.state.FunctionInitializationContext;
import org.apache.flink.runtime.state.FunctionSnapshotContext;
import org.apache.flink.streaming.api.checkpoint.CheckpointedFunction;
import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.HashMap;
import java.util.Properties;

public class KafkaUpsertSink extends RichSinkFunction<HashMap> implements CheckpointedFunction {


    KafkaProducer<String, String> producer = null;

    @Override
    public void open(Configuration parameters) throws Exception {
        Properties properties = new Properties();
        properties.setProperty("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        properties.put("bootstrap.servers", "");
        properties.put("enable.auto.commit ", "false");

        producer = new KafkaProducer<String, String>(properties);

    }

    @Override
    public void snapshotState(FunctionSnapshotContext context) throws Exception {

    }

    @Override
    public void initializeState(FunctionInitializationContext context) throws Exception {

    }

    @Override
    public void invoke(HashMap value, Context context) throws Exception {
        ProducerRecord<String, String> kafkaMessage = null;
        HashMap recordKeyMap = new HashMap();
        HashMap recordValueMap = new HashMap();
        HashMap keyMap = new HashMap();
        keyMap.put("db", value.get("db").toString());
        keyMap.put("table", value.get("table").toString());
        keyMap.put("id", value.get("id").toString());
        recordKeyMap.put("record_key",keyMap);
        recordValueMap.put("record_key",keyMap);
        recordValueMap.put("record_value",value);
        String recordKey = JSON.toJSONString(recordKeyMap);
        if (value.get("operation").toString().equals("d")) {
            kafkaMessage = new ProducerRecord("topic", recordKey, null);
            System.out.println("Produce ok:" + producer.send(kafkaMessage).get().toString());
        } else if (value.get("operation").toString().equals("u")) {
            //先发tombstone消息
            kafkaMessage = new ProducerRecord("topic", recordKey, null);
            System.out.println("Produce ok:" + producer.send(kafkaMessage).get().toString());
            //再发更改后消息
            kafkaMessage = new ProducerRecord("topic", recordKey, JSON.toJSONString(recordValueMap));
            System.out.println("Produce ok:" + producer.send(kafkaMessage).get().toString());
        } else {
            kafkaMessage = new ProducerRecord("topic", recordKey, JSON.toJSONString(recordValueMap));
            System.out.println("Produce ok:" + producer.send(kafkaMessage).get().toString());
        }
    }

}
```

## kafka数据sink hbase
### 1、创建upsert kafka table
```sql
REATE TABLE cdc_table (
  record_key STRING,
  record_value STRING,
  PRIMARY KEY (record_key) NOT ENFORCED
) WITH (
  'connector' = 'upsert-kafka',
  'topic' = '',
  'properties.bootstrap.servers' = '',
  'key.format' = 'json',
  'value.format' = 'json',
  'key.json.ignore-parse-errors' = 'true',
  'value.json.ignore-parse-errors' = 'true'
);

```
### 2、在hbase和flink sql catalog中创建与mysql维度表一一对应的hbase表
hbase表创建此处不赘述，flink sql catalog建表语句如下：
```sql
CREATE TABLE gate_info_hbase (
 entity_gate STRING,
 tb_column1 ROW<brand STRING,entity_id STRING,gate_id STRING,gate_name STRING,life_start_date STRING,life_end_date STRING,data_status STRING,parent_area_id STRING,data_checked STRING,parent_floor_id STRING>,
 PRIMARY KEY (entity_gate) NOT ENFORCED
) WITH (
 'connector' = 'hbase-1.4',
 'table-name' = 'gate_info_hbase',
 'zookeeper.quorum' = ''
);
```
### 3、将kafka数据导入hbase 
使用flink sql实现，语句如下
```sql
insert into mall_gate_hbase
select CONCAT(entity_id,'#',gate_id),ROW(brand,entity_id,gate_id,gate_name,life_start_date,life_end_date,data_status,parent_area_id,data_checked,parent_floor_id)
from (
select GetJsonString(record_value,'brand') as brand,GetJsonString(record_value,'entity_id') as entity_id,
GetJsonString(record_value,'gate_id') as gate_id,GetJsonString(record_value,'gate_name') as gate_name,
GetJsonString(record_value,'life_start_date') as life_start_date,GetJsonString(record_value,'life_end_date') as life_end_date,
GetJsonString(record_value,'status') as data_status,GetJsonString(record_value,'parent_area_id') as parent_area_id,
GetJsonString(record_value,'checked') as data_checked,GetJsonString(record_value,'parent_floor_id') as parent_floor_id
from cdc_table
where GetJsonString(record_value,'table')='mall_gate';
) a 
```
## 事件表和hbase维度表关联
使用flink sql实现，代码如下
```sql
kafka事件表建表sql，此处使用基于处理时间的join，所以事件表需要有cur_time AS PROCTIME()字段

CREATE TABLE person_times_event_log ( 
    request_id VARCHAR, 
    site_id VARCHAR, 
    `type` VARCHAR, 
    area_id VARCHAR, 
    gate_id VARCHAR, 
    store_id VARCHAR,
    `floor` VARCHAR, 
    event_type VARCHAR, 
    camera_id VARCHAR, 
    send_time BIGINT, 
    `timestamp` BIGINT,
    cur_time AS PROCTIME() 
) WITH ( 
    'connector.type' = 'kafka', -- 使用 kafka connector 
    'connector.version' = 'universal',  -- kafka 版本，universal 支持 0.11 以上的版本 
    'connector.topic' = '',  -- kafka topic 
    'connector.startup-mode' = 'earliest-offset', 
    'connector.properties.bootstrap.servers' = '', 
    'format.type' = 'json'
);

基于处理时间的动态表join计算人次


select ExtractDate(person_times_event_log.`timestamp`,'yyyy-MM-dd'),count(1) as daily_person_times
from person_times_event_log
join gate_info_hbase FOR SYSTEM_TIME AS OF person_times_event_log.cur_time  
on concat(person_times_event_log.site_id,'#',person_times_event_log.gate_id) = gate_info_hbase.entity_gate
group by ExtractDate(person_times_event_log.`timestamp`,'yyyy-MM-dd');
```


代码地址：https://github.com/cuiwenxu/flink-sql-demo



