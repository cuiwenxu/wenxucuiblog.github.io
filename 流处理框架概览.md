@[TOC](流处理框架概览)
# storm(record ack容错)
虽然流处理多年来一直广泛应用于金融等行业，但它最近正成为更广泛的用例的数据基础设施的一部分。开源框架的可用性推动了这种采用。也许开源世界中第一个广泛使用的大规模流处理框架是Apache Storm。Storm使用一种上游备份和记录确认机制来保证消息在失败后被重新处理。需要注意的是，Storm并不保证状态的一致性，任何可变状态的处理都是委托给用户的(Storm的Trident API保证了状态的一致性，将在下一节中进行描述)。

确认的工作原理如下:从操作符处理的每条记录都向前一个操作符发送一个确认，确认它已经被处理。拓扑源保留了它生成的所有元组的备份。一旦一个源记录收到了所有生成记录的确认，直到sink，它就可以从上游备份中安全地丢弃。失败时，如果没有收到所有确认，则会重播源记录。这保证了没有数据丢失，但是会导致重复的记录通过系统(因此术语“至少一次”)。Storm用一个聪明的机制实现了这个方案，每个源记录只需要几个字节来跟踪确认。Twitter Heron维护了与Storm相同的确认机制，但是提高了记录重放的效率(以及恢复时间和总体吞吐量)。

纯记录确认体系结构，无论其性能如何，都无法提供准确的一次保证，这给应用程序开发人员增加了重复数据删除的负担。这对于某些应用程序可能是可以接受的，但对于其他许多应用程序则不行。Storm机制的其他问题是低吞吐量和流量控制问题，因为确认机制经常在背压下错误地对故障进行分类。这导致了基于微批处理的流架构的下一个演变。
# 微批处理 (Trident, Spark Streaming)
Storm不提供一些对大规模应用程序至关重要的需求，特别是高吞吐量、快速并行恢复和托管状态的精确一次语义。

容错流架构的下一个发展是微批处理。其思想非常简单:为了克服处理和缓冲记录的连续操作符模型所带来的记录级同步的复杂性和开销，连续计算被分解为一系列小的、原子性的批处理作业(称为微批处理)。每个微批可能成功或失败。在失败时，可以简单地重新计算最新的微批次。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210519212600930.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE2MjQxNTc=,size_16,color_FFFFFF,t_70)


微批处理技术可以应用于能够进行数据流计算的现有引擎之上。例如,micro-batching可以应用在批处理引擎(例如spark),它也可以应用在流引擎(如storm)提供恰好一次准确性保障和快速并行恢复。在Spark Streaming中，每个微批处理计算都是一个Spark task，而在Trident中，每个微批处理都是一个大记录，该微批处理中的所有记录都被折叠到其中。

基于微批处理的系统可以实现上面列出的很多要求(精确一次保证，高吞吐量)，但它们还有很多有待改进的地方:
- 编程模型:为了实现其目标，例如Spark Streaming将编程模型从流转换为微批处理。`这意味着，除了检查点间隔的倍数外，用户不能再使用窗口数据`，并且模型不能支持许多应用程序需要的基于计数的或会话窗口。这些问题留给应用程序开发人员考虑。具有连续操作符的纯流模型为用户提供了更大的灵活性。

- 流量控制:使用基于时间的批处理的微批处理体系结构存在反压效应的固有问题。如果微批处理在下游操作(例如，由于计算密集型操作符，或缓慢的接收器)中花费的时间比批处理操作符(通常是源)更长，那么微批处理的时间将比配置的更长。这要么导致越来越多的批排队，要么导致微批规模不断增长。

- 延迟:微批处理显然将作业的延迟限制为微批处理的延迟。虽然次秒级的批处理延迟对于简单的应用程序是可行的，但具有多个网络shuffle的应用程序很容易将延迟提高到多秒。

`微批处理模型的最大限制可能是它连接了两个不应该连接的概念:应用程序定义的窗口大小和系统内部恢复间隔。`即窗口大小必须=系统恢复时间间隔的整数倍。

# flink(分布式一致性快照容错)
提供精确的一次保证的问题实际上归结为确定流计算当前处于什么状态(包括正在运行的记录和操作符状态)，绘制该状态的一致快照，并将该快照存储在持久存储中。如果可以经常这样做，那么从故障中恢复仅仅意味着从持久存储中恢复最新的快照，重新卷回流源(例如在Apache Kafka的帮助下)，直到快照被捕获并再次点击播放按钮。本文描述了Flink算法;下面，我们做一个简要的总结。

Flink的快照算法是基于Chandy和Lamport在1985年引入的一种技术，该技术可以绘制分布式系统当前状态的一致快照(请参阅这里的介绍)，不会丢失信息，也不会记录重复。Flink是Chandy Lamport算法的变体，它定期绘制一个运行中的流拓扑的状态快照，并将这些快照存储到持久存储中(例如，到HDFS或内存中的文件系统中)。这些检查点的频率是可配置的。

这类似于微批处理方法，在这种方法中，两个检查点之间的所有计算作为一个整体原子地成功或失败。然而，相似之处仅止于此。Chandy Lamport的一个伟大特性是，我们永远不必在流处理中按“暂停”按钮来安排下一个微批次。相反，常规数据处理总是继续进行，在事件出现时进行处理，而检查点则在后台进行。引用原文：
>全局状态检测算法将叠加在底层计算上:它必须与底层计算同时运行，但不能改变底层计算。

因此，该体系结构结合了遵循真正的连续操作符模型(低延迟、流控制和真正的流编程模型)和高吞吐量的优点，并且可以由Chandy-Lamport算法证明恰好是一次性的保证。除了备份有状态计算的状态(其他所有容错机制也需要这样做)之外，这种容错机制几乎没有任何开销。对于较小的状态(例如，计数或其他统计摘要)，这种备份开销通常可以忽略不计，而对于较大的状态，检查点间隔会在吞吐量和恢复时间之间进行权衡。

最重要的是，该体系结构将应用程序开发与流控制和吞吐量控制分离开来。更改快照间隔对流作业的结果绝对没有影响，因此下游应用程序可以安全地依赖于接收正确的结果。

Flink的检查点机制是基于barrier实现。Flink检查点的描述改编自Flink文档。


barrier首先注入到源端，并与数据记录一起作为数据流的一部分流经DAG。barrier将记录分成两组:属于当前快照的记录(barrier标志着检查点的开始)，以及属于下一个快照的记录。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210519214157783.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE2MjQxNTc=,size_16,color_FFFFFF,t_70)
barrier流向下游，并在通过operator时触发状态快照。operator首先从所有传入流分区对齐它的barrier(如果operator有多个输入)，缓冲来自更快channel的数据。当operator从每个传入流接收到一个barrier时，它将检查其状态(如果有的话)到持久存储。一旦完成状态检查点，操作符将barrier转发到下游。注意，在这种机制中，状态检查点可以是异步的(在写入状态时继续处理)，也可以是增量的(只写入更改)，如果operator支持的话。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210519214433527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE2MjQxNTc=,size_16,color_FFFFFF,t_70)

一旦所有数据接收器都接收到了barrier，当前的检查点就完成了。故障恢复意味着简单地恢复最新的检查点状态，并从最近记录的barrier重新启动源。分布式快照可以一次性保证高吞吐量，同时保留连续操作符模型，以及低延迟和自然流控制。


